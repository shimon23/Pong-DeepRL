{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warning messages.\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import random\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import gym # Game environment.\n",
    "import numpy as np  # Handle matrices.\n",
    "import pickle # Save and restore data package.\n",
    "from collections import deque # For stacking states.\n",
    "\n",
    "import tensorflow as tf  # Deep Learning library.\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "\n",
    "# Import my functions and classes:\n",
    "import DQNetwork as DNQ\n",
    "import preFunctions as pre\n",
    "import Memory as Mem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 4 # Our vector size.\n",
    "original_state_size = (210, 160, 3)\n",
    "stack_size = 4 # stack with 4 states.\n",
    "stack_states_size = [stack_size,state_size] # The size of the input to neural network.\n",
    "batch_size = 64  # Batch size.\n",
    "\n",
    "action_size = 6  # Actions: [stay,stay,up,down,up,down]\n",
    "possible_actions = [[1,0,0,0,0,0],[0,1,0,0,0,0],[0,0,1,0,0,0],[0,0,0,1,0,0],[0,0,0,0,1,0],[0,0,0,0,0,1]]\n",
    "\n",
    "learning_rate = 0.00001  # Alpha(learning rate).\n",
    "gamma = 0.99  # Discounting rate.\n",
    "\n",
    "total_episodes = 5000  # Total episodes for training.\n",
    "max_steps = 50000  # Max possible steps in an episode.\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0  # exploration probability at start\n",
    "explore_stop = 0.01  # minimum exploration probability\n",
    "decay_rate = 0.00000001  # exponential decay rate for exploration prob\n",
    "\n",
    "pretrain_length = batch_size  # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000  # Number of experiences the Memory can keep\n",
    "\n",
    "rewards_list = [] # list of all training rewards.\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "# training = False\n",
    "\n",
    "### MODIFY THIS TO FALSE IF IS NOT THE FIRST TARINING EPISODE.\n",
    "# firstTrain = True\n",
    "firstTrain = False\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "# episode_render = True\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "    \n",
    "    \n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name=\"actions_\")\n",
    "\n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "\n",
    "            self.flatten = tf.contrib.layers.flatten(self.inputs_)\n",
    "\n",
    "            # # First layer:\n",
    "            self.W1 = tf.Variable(tf.contrib.layers.xavier_initializer()((16, 256)))\n",
    "            self.b1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b1\")\n",
    "            self.z1 = tf.nn.relu(tf.matmul(self.flatten, self.W1) + self.b1, name=\"z1\")\n",
    "\n",
    "            # # Second layer:\n",
    "            self.W2 = tf.Variable(tf.contrib.layers.xavier_initializer()((256, 6)))\n",
    "            # self.W2 = tf.Variable(tf.truncated_normal([1024,6], stddev=0.1), name=\"W2\")\n",
    "            self.b2 = tf.Variable(tf.constant(0.1, shape=[6]), name=\"b2\")\n",
    "            self.z2 = tf.matmul(self.z1, self.W2) + self.b2\n",
    "\n",
    "            self.output = self.z2\n",
    "\n",
    "            # Q is our predicted Q value.\n",
    "            # result = double\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output,self.actions_))\n",
    "\n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q-self.Q))\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiences memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "\n",
    "    # Init deque for the memory:\n",
    "    def __init__(self,max_size):\n",
    "        self.buffer = deque(maxlen= max_size)\n",
    "\n",
    "    # Add experience to memory:\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    # Take random batch_size experiences from memory:\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),size=batch_size,replace=False)\n",
    "\n",
    "        return [self.buffer[i] for i in index]\n",
    "\n",
    "    # Get all experiences:\n",
    "    def getAllMemory(self):\n",
    "        return self.buffer\n",
    "\n",
    "    # Get the size of the memory:\n",
    "    def getMemorySize(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    # Get max size of the memory:\n",
    "    def getCapacity(self):\n",
    "        return self.buffer.maxlen\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State to vector function:\n",
    "# Argument: state - matrix of pixels.\n",
    "# Reuturn: vector of [P1,P2,xBall,yBall,mX,mY,speed]\n",
    "def stateToVector(state):\n",
    "    # [P1,P2,xBall,yBall]\n",
    "    vector = [0, 0, 0, 0]\n",
    "\n",
    "    # player1(left) position:\n",
    "    for i in range(34, 194):\n",
    "        if (state[i][16][0] == 213):\n",
    "            if(i == 16):\n",
    "                for i2 in range(34,51):\n",
    "                    if(state[i2][16][0] == 213 and state[i2+1][16][0] == 144):\n",
    "                        vector[0] = i2 - 16 + 1\n",
    "            else:\n",
    "                vector[0] = i\n",
    "            break\n",
    "\n",
    "    # # player2(right) position:\n",
    "    for i in range(34, 194):\n",
    "        if (state[i][140][0] == 92):\n",
    "            if(i == 34):\n",
    "                for i2 in range(34,51):\n",
    "                    if(state[i2][140][0] == 92 and state[i2+1][140][0] == 144):\n",
    "                        vector[1] = i2 - 16 + 1\n",
    "            else:\n",
    "                vector[1] = i\n",
    "            break\n",
    "\n",
    "    # Ball position:\n",
    "    for i in range(34, 194):\n",
    "        for j in range(0, 160):\n",
    "            if (state[i][j][0] == 236):\n",
    "                # print(\"Ball: x=\", i, \",y=\", j)\n",
    "                ball = (i, j)\n",
    "                vector[2] = i\n",
    "                vector[3] = j\n",
    "                break\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "# Get time vector:\n",
    "# Argument: counter of secondes from the starting tarining.\n",
    "# Return: vector of: [DAYS,HOURS,MINUTES,SECONDES].\n",
    "def getTime(counter):\n",
    "    time = []\n",
    "\n",
    "    day = counter // (24 * 3600)\n",
    "    time.append(day)\n",
    "\n",
    "    counter = counter % (24 * 3600)\n",
    "    hour = counter // 3600\n",
    "    time.append(hour)\n",
    "\n",
    "    counter %= 3600\n",
    "    minutes = counter // 60\n",
    "    time.append(minutes)\n",
    "\n",
    "    seconds = counter % 60\n",
    "    time.append(seconds)\n",
    "\n",
    "    return time\n",
    "\n",
    "\n",
    "# Predict action function: predict the next action:\n",
    "# Arguments: 1. sess - tensorflow session.\n",
    "#            2. DQNetwork2 - neural network model.\n",
    "#            3. explore_start - 1.0(const), for epsilon greedy strategy.\n",
    "#            4. explore_stop - 0.1(const), for epsilon greedy strategy.\n",
    "#            5. decay_rate - variable, for reducing the selection of a random step during the game.\n",
    "#            6. decay_step - variable, for reducing the selection of a random step during the game.\n",
    "#            7. state - matrix/vector of the current state.\n",
    "#            8. actions - possible actions.\n",
    "# Return: 1. action - the predicted action.\n",
    "#         2. explore_probability - the current probability for taking random action.\n",
    "def predict_action(sess, DQNetwork2, explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    \n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.randint(1, len(actions)) - 1\n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        state = np.array(state)\n",
    "        Qs = sess.run(DQNetwork2.output, feed_dict={DQNetwork2.inputs_: state.reshape((1, *state.shape))})\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        action = np.argmax(Qs)\n",
    "        # print(action)\n",
    "\n",
    "    return action, explore_probability\n",
    "\n",
    "\n",
    "# Print the action(DOWN,UP,STAY):\n",
    "# Argument: action - 0/1/2/3/4/5\n",
    "def actionToString(action):\n",
    "    if(action==1 or action==0):\n",
    "        print(\"STAY\")\n",
    "    elif(action==2 or action==4):\n",
    "        print(\"UP\")\n",
    "    elif(action== 3 or action==5):\n",
    "        print(\"DOWN\")\n",
    "\n",
    "\n",
    "\n",
    "# stack_states function:\n",
    "# Arguments: 1. stacked_vectors - (deque) deque with 4 vectors.\n",
    "#            2. state - (matrix) vector of current state.\n",
    "#            3. is_new_episode - (boolean) check if we start an new episode.\n",
    "#            4. stack_size - (int).\n",
    "# Return: 1. stacked_state - (numpy stack).\n",
    "#         2. stacked_vectors - (deque)\n",
    "def stack_states(stacked_vectors, state, is_new_episode,stack_size,state_size):\n",
    "    # Preprocess frame\n",
    "    stateVec = stateToVector(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_vectors = deque([np.zeros((state_size), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "        # Because we're in a new episode, copy the same state 4x\n",
    "        stacked_vectors.append(stateVec)\n",
    "        stacked_vectors.append(stateVec)\n",
    "        stacked_vectors.append(stateVec)\n",
    "        stacked_vectors.append(stateVec)\n",
    "\n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_vectors)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_vectors.append(stateVec)\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_vectors)\n",
    "\n",
    "    return stacked_state, stacked_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/netanelhugi/.local/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/netanelhugi/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Create our environment:\n",
    "env = gym.make('Pong-v0')\n",
    "\n",
    "# Create log file:\n",
    "text_file = open(\"./saveData/log.txt\", \"a\")\n",
    "\n",
    "# Initialize deque with zero-vectors states.\n",
    "stacked_vectors  =  deque([np.zeros((state_size), dtype=np.float) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork2 = DNQ.DQNetwork(stack_states_size, action_size, learning_rate)\n",
    "\n",
    "# Instantiate memory\n",
    "memory = Mem.Memory(max_size=memory_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/netanelhugi/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Ep: 904 ,Total time: D:0,H:16,M:15,S:36\n",
      "Episode: 904 Total reward: -21.0 Explore P: 0.0965 Training Loss 3.9756898880004883\n",
      "Model Saved\n",
      "Ep: 905 ,Total time: D:0,H:16,M:16,S:23\n",
      "Episode: 905 Total reward: -21.0 Explore P: 0.0965 Training Loss 32.06666564941406\n",
      "Ep: 906 ,Total time: D:0,H:16,M:17,S:17\n",
      "Episode: 906 Total reward: -21.0 Explore P: 0.0965 Training Loss 1.0445690155029297\n",
      "Ep: 907 ,Total time: D:0,H:16,M:18,S:8\n",
      "Episode: 907 Total reward: -21.0 Explore P: 0.0965 Training Loss 0.1182343065738678\n",
      "Ep: 908 ,Total time: D:0,H:16,M:18,S:56\n",
      "Episode: 908 Total reward: -21.0 Explore P: 0.0965 Training Loss 23.203773498535156\n",
      "Ep: 909 ,Total time: D:0,H:16,M:19,S:40\n",
      "Episode: 909 Total reward: -21.0 Explore P: 0.0965 Training Loss 0.05123654752969742\n",
      "Ep: 910 ,Total time: D:0,H:16,M:20,S:33\n",
      "Episode: 910 Total reward: -21.0 Explore P: 0.0965 Training Loss 2.7909340858459473\n",
      "Ep: 911 ,Total time: D:0,H:16,M:21,S:17\n",
      "Episode: 911 Total reward: -21.0 Explore P: 0.0965 Training Loss 20.434799194335938\n",
      "Ep: 912 ,Total time: D:0,H:16,M:22,S:8\n",
      "Episode: 912 Total reward: -20.0 Explore P: 0.0965 Training Loss 0.10816512256860733\n",
      "Ep: 913 ,Total time: D:0,H:16,M:22,S:59\n",
      "Episode: 913 Total reward: -21.0 Explore P: 0.0965 Training Loss 0.1505945920944214\n",
      "Ep: 914 ,Total time: D:0,H:16,M:23,S:47\n",
      "Episode: 914 Total reward: -21.0 Explore P: 0.0965 Training Loss 44.42382049560547\n",
      "Model Saved\n",
      "Ep: 915 ,Total time: D:0,H:16,M:24,S:35\n",
      "Episode: 915 Total reward: -20.0 Explore P: 0.0964 Training Loss 0.5870617032051086\n",
      "Ep: 916 ,Total time: D:0,H:16,M:25,S:26\n",
      "Episode: 916 Total reward: -21.0 Explore P: 0.0964 Training Loss 10.69710636138916\n",
      "Ep: 917 ,Total time: D:0,H:16,M:26,S:13\n",
      "Episode: 917 Total reward: -21.0 Explore P: 0.0964 Training Loss 4.903686046600342\n",
      "Ep: 918 ,Total time: D:0,H:16,M:27,S:2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If is our first training episode:\n",
    "if(firstTrain):\n",
    "    # Create log file:\n",
    "    text_file = open(\"log.txt\", \"w\")\n",
    "\n",
    "    # Init memory with states:\n",
    "    for i in range(pretrain_length):\n",
    "        # If it's the first step\n",
    "        if i == 0:\n",
    "            state = env.reset()\n",
    "            state, stacked_vectors = pre.stack_states(stacked_vectors, state, True,stack_size,state_size)\n",
    "\n",
    "        # Get the next_state, the rewards, done by taking a random action\n",
    "        action = random.randint(1, len(possible_actions)) - 1\n",
    "        # action = pre.actionAdapter(choice)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state, stacked_vectors = pre.stack_states(stacked_vectors, next_state, False,stack_size,state_size)\n",
    "\n",
    "        # If the episode is finished (until we get 21)\n",
    "        if done:\n",
    "            # We finished the episode\n",
    "            next_state = np.zeros(state.shape)\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, possible_actions[action], reward, next_state, done))\n",
    "            # Start a new episode\n",
    "            state = env.reset()\n",
    "            state, stacked_vectors = pre.stack_states(stacked_vectors, state, True,stack_size,state_size)\n",
    "\n",
    "\n",
    "        else:\n",
    "            # append to log file:\n",
    "            text_file = open(\"./saveData/log.txt\", \"a\")\n",
    "\n",
    "            # Add experience to memory\n",
    "            memory.add((state, possible_actions[action], reward, next_state, done))\n",
    "            # Our new state is now the next_state\n",
    "            state = next_state\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# If we continue with the training:\n",
    "else:\n",
    "    # restore memory data:\n",
    "    with open(\"./saveData/memory.dq\", \"rb\") as fp:\n",
    "        temp = pickle.load(fp)\n",
    "\n",
    "    # Add to memory buffer:\n",
    "    for i in temp:\n",
    "        memory.add(i)\n",
    "\n",
    "\n",
    "\n",
    "# Tensorflow variables for save:\n",
    "\n",
    "# Episodes counter:\n",
    "episodeCounter = tf.Variable(1)\n",
    "step = tf.constant(1)\n",
    "update = tf.assign(episodeCounter, episodeCounter + step)\n",
    "\n",
    "# TIme counter:\n",
    "secondsCounter = tf.Variable(.0)\n",
    "\n",
    "# Initialize the decay rate (that will use to reduce epsilon)\n",
    "decay_step = tf.Variable(0)\n",
    "decay_stepVar = 0\n",
    "\n",
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Training mode:\n",
    "if training == True:\n",
    "    env = gym.wrappers.Monitor(env, \"./vid\", video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        if(firstTrain==False):\n",
    "            # Load the model and the variables\n",
    "            saver.restore(sess, \"./models/model.ckpt\")\n",
    "        else:\n",
    "            # Initialize the variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            startTimeEp = time.time() # Start episode time.\n",
    "\n",
    "            # Get and print total training time:\n",
    "            timeVector = pre.getTime(sess.run(secondsCounter))\n",
    "            print(\"Ep: %d\" %sess.run(episodeCounter),\",Total time: D:%d,H:%d,M:%d,S:%d\"%(int(timeVector[0]),int(timeVector[1]),int(timeVector[2]),int(timeVector[3])))\n",
    "\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "\n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "\n",
    "            # Record episodes:\n",
    "\n",
    "            # Make a new episode and observe the first state\n",
    "            state = env.reset()\n",
    "            state, stacked_vectors = pre.stack_states(stacked_vectors, state, True,stack_size,state_size)\n",
    "\n",
    "            while step < max_steps:\n",
    "                # Increase decay_step\n",
    "                decay_stepVar += 1\n",
    "\n",
    "                # Predict the next action:\n",
    "                action, explore_probability = pre.predict_action(sess, DQNetwork2, explore_start, explore_stop,\n",
    "                                                                 decay_rate,\n",
    "                                                                 sess.run(decay_step), state,\n",
    "                                                                 possible_actions)\n",
    "                # Perform the action and get the next_state, reward, and done information\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Game display:\n",
    "                if episode_render:\n",
    "                    env.render()\n",
    "\n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # The episode ends so no next state\n",
    "                    next_state = np.zeros(original_state_size, dtype=np.int)\n",
    "                    next_state, stacked_vectors = pre.stack_states(stacked_vectors, next_state, False,stack_size,state_size)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    # Print episode summery:\n",
    "                    print('Episode: {}'.format(sess.run(episodeCounter)),\n",
    "                          'Total reward: {}'.format(total_reward),\n",
    "                          'Explore P: {:.4f}'.format(explore_probability),\n",
    "                          'Training Loss {}'.format(loss))\n",
    "\n",
    "                    # Send the summery to log file:\n",
    "                    str2 = \"Episode: \" + str(sess.run(episodeCounter)) + \", Total reward:\"+str(total_reward) + \", Explore P: \"+ str(explore_probability)+ \", loss: \"+str(loss) + \"\\n\"\n",
    "                    text_file.write(str2)\n",
    "\n",
    "                    # Add reward to total rewards list:\n",
    "                    rewards_list.append((episode, total_reward))\n",
    "\n",
    "                    # Store transition <st,at,rt+1,st+1> in memory D\n",
    "                    memory.add((state, possible_actions[action], reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    next_state, stacked_vectors = pre.stack_states(stacked_vectors, next_state, False,stack_size,state_size)\n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, possible_actions[action], reward, next_state, done))\n",
    "\n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "                ### LEARNING PART\n",
    "                #Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                # print(batch)\n",
    "                states_mb = np.array([each[0] for each in batch],ndmin=2)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch],ndmin=2)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                # Get Q values for next_state\n",
    "                Qs_next_state = sess.run(DQNetwork2.output, feed_dict={DQNetwork2.inputs_: next_states_mb})\n",
    "\n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _,Q= sess.run([DQNetwork2.loss, DQNetwork2.optimizer,DQNetwork2.Q],\n",
    "                                   feed_dict={DQNetwork2.inputs_: states_mb,\n",
    "                                              DQNetwork2.target_Q: targets_mb,\n",
    "                                              DQNetwork2.actions_: actions_mb})\n",
    "\n",
    "            # Update episode number:\n",
    "            sess.run(update)\n",
    "\n",
    "            # Time update:\n",
    "            endTimeEp = time.time()\n",
    "            timeUpdate = tf.assign_add(secondsCounter, endTimeEp - startTimeEp)\n",
    "            sess.run(timeUpdate)\n",
    "\n",
    "            # Decay update:\n",
    "            decayStepUpdate = tf.assign_add(decay_step, decay_stepVar)\n",
    "            sess.run(decayStepUpdate)\n",
    "\n",
    "\n",
    "            # Save model every 10 episodes\n",
    "            if episode % 10 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")\n",
    "\n",
    "                # Save memory data:\n",
    "                with open(\"./saveData/memory.dq\", \"wb\") as fp:  # Pickling\n",
    "                    pickle.dump(memory.getAllMemory(), fp)\n",
    "\n",
    "\n",
    "            # Test every 10 episodes:\n",
    "            # if episode % 50 == 0:\n",
    "                total_test_rewards = []\n",
    "\n",
    "                total_rewards = 0\n",
    "\n",
    "                state = env.reset()\n",
    "                state, stacked_vectors = pre.stack_states(stacked_vectors, state, True, stack_size, state_size)\n",
    "\n",
    "\n",
    "                # print(\"****************************************************\")\n",
    "                # print(\"TEST EPISODE: \",sess.run(episodeCounter))\n",
    "                # \n",
    "                # while True:\n",
    "                #     stateArr = []\n",
    "                #     stateArr.append(state)\n",
    "                #\n",
    "                #     # Get action from Q-network\n",
    "                #     # Estimate the Qs values state\n",
    "                #     Qs = sess.run(DQNetwork2.output, feed_dict={DQNetwork2.inputs_: stateArr})\n",
    "                #\n",
    "                #     # Take the biggest Q value (= the best action)\n",
    "                #     action = np.argmax(Qs[0])\n",
    "                #\n",
    "                #     # Perform the action and get the next_state, reward, and done information\n",
    "                #     next_state, reward, done, _ = env.step(action)\n",
    "                #     env.render()\n",
    "                #\n",
    "                #     total_rewards += reward\n",
    "                #\n",
    "                #     if done:\n",
    "                #         print(\"Score\", total_rewards)\n",
    "                #         total_test_rewards.append(total_rewards)\n",
    "                #         break\n",
    "                #\n",
    "                #     next_state, stacked_vectors = pre.stack_states(stacked_vectors, next_state, False, stack_size,\n",
    "                #                                                    state_size)\n",
    "                #     state = next_state\n",
    "                #\n",
    "                # # Update episode number:\n",
    "                # sess.run(update)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Testing mode:\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    env = gym.wrappers.Monitor(env, \"./vid\", video_callable=lambda episode_id: True, force=True)\n",
    "\n",
    "\n",
    "    total_test_rewards = []\n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    for episode in range(20):\n",
    "        total_rewards = 0\n",
    "\n",
    "        state = env.reset()\n",
    "        state, stacked_vectors = pre.stack_states(stacked_vectors, state, True,stack_size,state_size)\n",
    "\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        while True:\n",
    "            stateArr = []\n",
    "            stateArr.append(state)\n",
    "\n",
    "            # Get action from Q-network\n",
    "            # Estimate the Qs values state\n",
    "            Qs = sess.run(DQNetwork2.output, feed_dict={DQNetwork2.inputs_: stateArr})\n",
    "\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            action = np.argmax(Qs[0])\n",
    "            # print(Qs)\n",
    "            # print(action)\n",
    "\n",
    "            # Perform the action and get the next_state, reward, and done information\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print(\"Score\", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break\n",
    "\n",
    "            next_state, stacked_vectors = pre.stack_states(stacked_vectors, next_state, False,stack_size,state_size)\n",
    "            state = next_state\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "text_file.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
